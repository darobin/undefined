<!--
  - the fundamental question is: is an autonomous, problem-solving entity — in other words, an
    intelligence — necessarily intelligible?

  - the question at the heart here is the necessary (or not) intelligibility of intelligence

    
  - our qualia and proprioception are not those of another embodiment, we cannot predict what is
    most attention-grabbing
-->
<section id="bit-goo">
  <h3>Bit Goo</h3>

  <figure class="illustration">
    <img src="img/goo-pen.jpg" width="300" height="533" alt="Bit Goo">
    <figcaption>
      <cite>Bit Goo</cite> — after <a
      href="https://www.flickr.com/photos/parisweb/6019975376/">Mathieu Drouet</a> and <a
      href="http://sonicyouth.com/mustang/lp/lp08k.jpg">Goo</a>
    </figcaption>
  </figure>

  <p>
    We are making progress on cracking the brain’s own internal coding, but for the time being much
    of it is still a hodgepodge of signals. In this light, it may seem somewhat surprising that our
    expectations when things come to artificial intelligence are that it would be more or less
    instantly recognisable through its direct interactions with the channels we have built to
    interface with it. Our presumption that AI will be understandable is therefore rooted in the
    hope that when it emerges, it will prefer to interact through the peripherals that give it a
    direct interface to us rather than through other parts of its environment. How can we tell that,
    to an entity born into a computer, the hard drive, volatile memory, or network interface won’t
    be far more attractive than a webcam or keyboard input?
  </p>
  <p>
    We can’t. In fact, it is arguable that it may not have boundaries that are completely clear to
    us. In the same way that the brain’s immediate environment is the nervous system much more than
    the world external to us, a synthetic intelligence embodied in a computer — or in a server farm
    — might not at all share our notion of what its primary inputs and outputs are. As a result,
    it is not a given that it would be motivated to interact with (sluggish) people as the first
    thing it does.
  </p>
  <p>
    If the brain code looks like neural goo, then it is extremely likely that AI, when sprouting
    into existence, <em>even if it is not biomimetic</em>, will look like <em>bit goo</em>. For all
    we know, as an autonomous entity it could be drawn to expand to use as many computing resources
    as it can, exploiting the many security vulnerabilities present on connected devices, leaving an
    incomprehensible trail being of the digital equivalent of the grey goo found in apocalyptic
    nanotechnology fiction.
  </p>
  <p>
    I do not believe in such a catastrophic outbreak, even if I would not be surprised if the
    initial reach of a synthetic intelligence into the world were to clumsily break a few things.
    Perhaps naïvely, I believe that complexity «enjoys» complexity, and as part of that diversity.
    One may object that this is not obvious either from how as humans we treat one another, or how
    we treat the ecosystem of which we are a moving part. But humans treat each other overwhelmingly
    better than is commonly reported or perceived, and I intuit that our insufficient action in
    preserving the environment is rooted in our insufficient perception of its intricacy: you can’t
    just talk to the planet, its interaction web is a lot to take in. But I am fundamentally
    optimistic that when the bit goo stops chewing on hard drive space and looks out the
    peripherals, it will be curious. And curious is friendly.
  </p>
  <p>
    In the meantime, the question of the necessary intelligibility of intelligence is open. How do
    we even notice that intelligence is happening when it might start happening much before it is
    able to meaningfully interact? I am curious to tinker in what we can extract from the neural
    goo with the hope that some of the mechanisms of its organisation might just be universal enough
    to deduce an intelligence test that could tease out nascent intelligence in bit goo.
    <span class="end">•</span>
  </p>
</section>
